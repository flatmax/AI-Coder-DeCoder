# Streaming Chat Protocol Spec

The streaming chat protocol covers the full lifecycle of a user message: from UI submission through message assembly, LLM streaming, edit parsing, stability tracking, and post-response compaction. It is the core request/response pipeline of the application.

## Architecture

```
User clicks Send
       â”‚
       â–¼
ChatActionsMixin.sendMessage()
       â”‚
       â”œâ”€ addMessage('user', text)          â† show in UI immediately
       â”œâ”€ _generateRequestId()              â† correlate callbacks
       â”œâ”€ _startStreamingWatchdog()         â† 5min safety timeout
       â”‚
       â–¼
LiteLLM.chat_streaming (JRPC)              â† returns {status: "started"} immediately
       â”‚
       â”œâ”€ store_user_message()              â† persist to history store
       â”œâ”€ asyncio.create_task(_stream_chat) â† background task
       â”‚
       â–¼
_stream_chat (async)
       â”‚
       â”œâ”€ Validate files
       â”œâ”€ Load files into FileContext
       â”œâ”€ Detect & fetch URLs from prompt
       â”œâ”€ _build_streaming_messages()       â† assemble tiered message array
       â”œâ”€ _run_streaming_completion()       â† threaded litellm.completion(stream=True)
       â”‚       â”‚
       â”‚       â”œâ”€ streamChunk callbacks â”€â”€â–º PromptView.streamChunk (JRPC callback)
       â”‚       â””â”€ return (full_content, was_cancelled)
       â”‚
       â”œâ”€ add_exchange to context manager
       â”œâ”€ _auto_save_symbol_map()
       â”œâ”€ _print_streaming_hud()
       â”œâ”€ Parse & apply edits
       â”œâ”€ store_assistant_message()
       â”œâ”€ _update_cache_stability()
       â”‚
       â–¼
_send_stream_complete â”€â”€â–º PromptView.streamComplete (JRPC callback)
       â”‚
       â–¼
_run_post_response_compaction (async, non-blocking)
       â”‚
       â”œâ”€ compactionEvent('compaction_start')
       â”œâ”€ compact_history_if_needed_sync()
       â””â”€ compactionEvent('compaction_complete')
```

## Client-Side Initiation

### `sendMessage()` (`ChatActionsMixin`)

1. **Guard** â€” Returns if input is empty and no pasted images.
2. **Reset scroll** â€” Sets `_userHasScrolledUp = false` so auto-scroll works.
3. **Capture state** â€” Saves `inputValue`, pasted images, and fetched URL content.
4. **Build URL context** â€” Calls `getFetchedUrlsForMessage()` to get included (non-excluded, non-errored) fetched URLs. Appends them as a `---\n**Referenced URL Content:**` section to the message sent to the LLM, but shows only the original text in the UI.
5. **Show user message** â€” `addMessage('user', userContent, images)` renders immediately.
6. **Clear input** â€” Resets `inputValue`, `pastedImages`, URL state (detected only, not fetched).
7. **Generate request ID** â€” `_generateRequestId()` returns `{timestamp}-{random9}`.
8. **Track request** â€” `_streamingRequests.set(requestId, {message})`.
9. **Set streaming state** â€” `isStreaming = true`, starts 5-minute watchdog.
10. **JRPC call** â€” `call['LiteLLM.chat_streaming'](requestId, message, selectedFiles, images)`.
11. **Handle sync error** â€” If the JRPC call itself returns `{error}`, updates the last assistant message with the error.

### Request ID

Format: `{Date.now()}-{Math.random().toString(36).substr(2, 9)}`

Used to correlate `streamChunk`, `streamComplete`, and `compactionEvent` callbacks with the originating request. The client ignores callbacks for unknown request IDs.

## Server-Side Entry Point

### `chat_streaming()` (`StreamingMixin`)

Synchronous method called via JRPC. Does three things:

1. **Store user message** â€” `store_user_message(content, images, files)` persists to `HistoryStore`.
2. **Launch background task** â€” `asyncio.create_task(_stream_chat(...))`.
3. **Return immediately** â€” `{"status": "started", "request_id": request_id}`.

Parameters:

| Param | Type | Description |
|-------|------|-------------|
| `request_id` | `str` | Client-generated correlation ID |
| `user_prompt` | `str` | User's message text (may include URL context) |
| `file_paths` | `list[str]?` | Selected files from file picker |
| `images` | `list[dict]?` | Base64-encoded images with `{data, mime_type}` |
| `use_smaller_model` | `bool` | Use `smaller_model` instead of `model` |
| `dry_run` | `bool` | Parse edits but don't write to disk |
| `use_repo_map` | `bool` | Include symbol map in context |

## Message Assembly

### `_build_streaming_messages()`

Assembles the full message array sent to the LLM. Content is organized into 5 stability tiers for prompt caching:

| Tier | Threshold | Cached | Description |
|------|-----------|--------|-------------|
| L0 | 12+ responses | Yes | Most stable â€” system prompt, legend, oldest symbols/files/history |
| L1 | 9+ responses | Yes | Very stable |
| L2 | 6+ responses | Yes | Stable |
| L3 | 3+ responses | Yes | Moderately stable â€” default tier for new items |
| active | <3 responses | No | Recently changed â€” not cached |

Each cache tier boundary is marked with `cache_control: {"type": "ephemeral"}` on the last message in the tier's sequence. This tells providers like Anthropic/Bedrock where to place cache breakpoints.

### Message Array Structure

The assembled message array follows this order:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. SYSTEM MESSAGE (L0)                              â”‚
â”‚    - System prompt (system.md + system_extra.md)     â”‚
â”‚    - Legend (path aliases for symbol map)            â”‚
â”‚    - L0 symbol map entries                          â”‚
â”‚    - L0 file contents                               â”‚
â”‚    [cache_control on last message in L0 sequence]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. L0 HISTORY (native user/assistant pairs)         â”‚
â”‚    [cache_control on last L0 history message]       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. L1 BLOCK (user/assistant pair)                   â”‚
â”‚    - L1 symbol map entries                          â”‚
â”‚    - L1 file contents                               â”‚
â”‚    - L1 history (native pairs)                      â”‚
â”‚    [cache_control on last message in L1 sequence]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. L2 BLOCK (same structure as L1)                  â”‚
â”‚    [cache_control on last message in L2 sequence]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. L3 BLOCK (same structure as L1)                  â”‚
â”‚    [cache_control on last message in L3 sequence]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6. FILE TREE (active, not cached)                   â”‚
â”‚    user: "# Repository Files\n..."                  â”‚
â”‚    assistant: "Ok."                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7. URL CONTEXT (active, not cached)                 â”‚
â”‚    user: "# URL Context\n..."                       â”‚
â”‚    assistant: "Ok, I've reviewed the URL content."  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 8. ACTIVE FILES (not cached)                        â”‚
â”‚    user: "# Working Files\nHere are the files:..."  â”‚
â”‚    assistant: "Ok."                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 9. ACTIVE HISTORY (native user/assistant pairs)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 10. USER MESSAGE (current turn)                     â”‚
â”‚    Plain text or multimodal with images             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System Prompt Construction

The system prompt is built by `build_system_prompt()` (`ac/prompts/loader.py`):

1. Load `config/prompts/system.md` â€” main prompt with role, symbol map instructions, edit protocol, workflow, and examples.
2. Load `config/prompts/system_extra.md` â€” optional additions (e.g., "Be lean", "Don't modify files not in context").
3. Concatenate with `\n\n` separator.

The system prompt is always the first content in the L0 tier.

### Symbol Map Organization

Symbol map entries use the `symbol:` prefix in the stability tracker (e.g., `symbol:ac/llm/llm.py`). Files in active context (selected in the file picker) are excluded from the symbol map because their full content replaces the summary.

Symbol map content is formatted by `format_symbol_blocks_by_tier()` which produces a separate formatted string per tier. Each tier's symbol content is preceded by a header:

- L0: `# Repository Structure\n\nBelow is a map...`
- L1â€“L3: `# Repository Structure (continued)\n\n`

Path aliases (e.g., `@1/=ac/symbol_index/extractors/`) are computed from frequently-referenced paths and included in the legend at the top of the L0 block.

### File Content Formatting

Files are formatted as:

```
# {Header}

These files are included for reference:

{path}
```{content}```

{path2}
```{content2}```
```

Headers vary by tier:

| Tier | Header |
|------|--------|
| L0 | `# Reference Files (Stable)` |
| L1 | `# Reference Files` |
| L2 | `# Reference Files (L2)` |
| L3 | `# Reference Files (L3)` |
| active | `# Working Files` |

### History Distribution

Conversation history messages are tracked as `history:{index}` in the stability tracker. Each message is sent as a native role/content pair (not wrapped in a formatted block), preserving the LLM's understanding of conversation turns.

History messages are distributed across tiers based on their stability. The `_get_history_tiers()` method:

1. Queries the stability tracker for all `history:{i}` items.
2. Groups them by tier, maintaining index order within each tier.
3. Any untracked messages go to `active`.

### Cache Control Placement

`_apply_cache_control(message)` modifies a message dict in-place:

- If content is already a list (structured), adds `cache_control` to the last `text` block.
- If content is a plain string, wraps it in `[{"type": "text", "text": content, "cache_control": {"type": "ephemeral"}}]`.

The placement strategy:

- **L0 without history** â€” `cache_control` on the system message itself.
- **L0 with history** â€” System message has no `cache_control`; it goes on the last L0 history message.
- **L1â€“L3** â€” `cache_control` on the last message in the combined sequence (symbols/files pair + history pairs).
- **Active** â€” No `cache_control` (uncached by design).

### Image Handling

If images are provided, the user message uses multimodal format:

```json
{
  "role": "user",
  "content": [
    {"type": "text", "text": "user prompt"},
    {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
  ]
}
```

## Streaming Execution

### `_run_streaming_completion()`

Runs synchronously in a thread pool executor (via `loop.run_in_executor`). This avoids blocking the asyncio event loop.

1. Calls `litellm.completion(model, messages, stream=True, stream_options={"include_usage": True})`.
2. Iterates over chunks.
3. For each chunk with content:
   - Appends to `full_content` (accumulating).
   - Calls `_fire_stream_chunk(request_id, full_content, loop)` â€” fire-and-forget.
4. Checks `_is_cancelled(request_id)` each iteration for cancellation.
5. After iteration, sends one final chunk with the complete content.
6. Tracks token usage from the final chunk's `usage` field.
7. Returns `(full_content, was_cancelled)`.

### Chunk Delivery

`_fire_stream_chunk()` schedules a coroutine on the main event loop from the worker thread via `asyncio.run_coroutine_threadsafe()`. Each chunk carries the **full accumulated content**, so:

- Dropped chunks are harmless (next chunk supersedes).
- Reordered chunks are harmless (latest content wins).
- Only `streamComplete` needs reliable delivery.

### Client Chunk Processing (`streamChunk`)

`StreamingMixin.streamChunk(requestId, content)`:

1. Ignores if `requestId` not in `_streamingRequests`.
2. Calls `streamWrite(content, false, 'assistant')` â€” delegates to `MessageHandler`.

`MessageHandler.streamWrite()` coalesces rapid chunks via `requestAnimationFrame`:

1. Stores pending chunk data.
2. On next animation frame, calls `_processStreamChunk()`.
3. Creates an assistant message on first chunk (if none exists).
4. Updates the last assistant message's content.
5. Triggers scroll-to-bottom.

## Cancellation

### Client

`stopStreaming()` calls `LiteLLM.cancel_streaming(requestId)` via JRPC.

### Server

`cancel_streaming(request_id)` adds the ID to a thread-safe `_cancelled_requests` set. The streaming thread checks `_is_cancelled()` on each chunk iteration and breaks out of the loop.

After cancellation:

- Partial content is stored with `*[stopped]*` appended.
- `streamComplete` is sent with `cancelled: true`.
- No edit parsing or application occurs.

## Stream Completion

### Server (`_send_stream_complete`)

Sends the final result via `PromptView.streamComplete(request_id, result)` JRPC callback with a 5-second timeout.

### Result Object

| Field | Type | Description |
|-------|------|-------------|
| `response` | `str` | Full assistant response text |
| `summarized` | `bool` | Always `false` (compaction is post-response) |
| `token_usage` | `dict` | Token counts (see HUD section) |
| `edit_format` | `str` | Always `"edit_v3"` |
| `edit_blocks` | `list[dict]` | Parsed blocks (preview, max 100/200 chars) |
| `shell_commands` | `list[str]` | Detected shell command suggestions |
| `passed` | `list[tuple]` | Successfully applied edits `(path, old_preview, new_preview)` |
| `failed` | `list[tuple]` | Failed edits `(path, reason, "")` |
| `skipped` | `list[tuple]` | Skipped edits `(path, reason, "")` |
| `files_modified` | `list[str]` | Paths of modified files |
| `edit_results` | `list[dict]` | Detailed results with `{file_path, status, reason, estimated_line, anchor_preview, old_preview, new_preview}` |
| `content` | `dict` | Empty (new parser writes directly to disk) |
| `cancelled` | `bool?` | Present and true if request was cancelled |
| `error` | `str?` | Present if a fatal error occurred |
| `binary_files` | `list?` | Files rejected as binary |
| `invalid_files` | `list?` | Files not found |

### Client (`streamComplete`)

`StreamingMixin.streamComplete(requestId, result)`:

1. **Flush pending chunks** â€” Processes any coalesced chunk that hasn't rendered yet.
2. **Clear state** â€” Removes request from `_streamingRequests`, sets `isStreaming = false`, clears watchdog.
3. **Handle errors** â€” Auto-deselects binary/invalid files from the file picker, shows error message.
4. **Finalize message** â€” Marks last assistant message as `final: true`, attaches `editResults`.
5. **Build edit results** â€” Prefers `edit_results` format; falls back to `passed`/`failed` tuples.
6. **Refresh file tree** â€” If edits were applied, calls `loadFileTree()` and dispatches `files-edited` event.
7. **Show HUD** â€” If `token_usage` present, calls `_showHud(tokenUsage)`.
8. **Refresh viewers** â€” Triggers cache viewer and context viewer refresh.
9. **Focus textarea** â€” After 100ms delay, focuses the input for next message.

## Edit Parsing & Application

After streaming completes (and if not cancelled):

1. **Parse** â€” `EditParser.parse_response(full_content)` extracts v3 edit blocks.
2. **Detect shell** â€” `EditParser.detect_shell_suggestions(full_content)` finds shell command suggestions.
3. **Apply** â€” If blocks found and not `dry_run`, `EditParser.apply_edits(blocks, repo)` writes changes to disk and stages files.
4. **Invalidate cache** â€” Modified files have their symbol index cache invalidated so references rebuild on next request.

## Post-Response Processing

### Context Manager Update

After streaming and edit application:

1. `_context_manager.add_exchange(user_text, full_content)` â€” Adds the exchange to conversation history.
2. `_auto_save_symbol_map()` â€” Re-indexes modified files.

### Cache Stability Update (`_update_cache_stability`)

Updates the stability tracker to determine tier assignments for the next request:

1. **Phase 0: Stale detection** â€” Finds tracked items whose files no longer exist. Removes them and marks their tiers as broken.
2. **Phase 1: File/symbol churn** â€” Identifies which file and symbol items are in active context.
3. **Phase 2: Controlled history graduation** â€” Determines which history messages should graduate from active to L3:
   - **Piggyback** â€” If L3 is already being invalidated (by file/symbol changes), history graduates for free.
   - **Threshold** â€” If eligible history tokens exceed `cache_target_tokens`, oldest messages graduate.
   - **Otherwise** â€” History stays active to avoid unnecessary cache churn.
4. **Phase 3: Update tracker** â€” Calls `stability.update_after_response()` with active items list, content hash callback, and modified items.
5. **Phase 4: Log** â€” Prints promotions (ğŸ“ˆ) and demotions (ğŸ“‰) to the console.

### History Storage

Both messages are persisted to `HistoryStore`:

- **User message** â€” Stored at the start of `chat_streaming()` (before the background task).
- **Assistant message** â€” Stored after edit application with `files_modified` and `edit_results` metadata.

## Post-Response Compaction

After `streamComplete` is sent, `_run_post_response_compaction()` runs asynchronously:

1. **Check** â€” `_context_manager.should_compact()` tests if history exceeds `compaction_trigger_tokens`.
2. **Delay** â€” 500ms sleep to let the frontend process `streamComplete`.
3. **Notify start** â€” `compactionEvent('compaction_start')` shows a "ğŸ—œï¸ Compacting history..." message.
4. **Compact** â€” Runs `compact_history_if_needed_sync()` in an executor thread.
5. **Re-register** â€” Removes old `history:*` entries from stability tracker. New entries register on next request.
6. **Notify complete** â€” `compactionEvent('compaction_complete')` with case, token counts, and compacted messages.

### Client Compaction Handling (`compactionEvent`)

| Event Type | Behavior |
|------------|----------|
| `compaction_start` | Shows "Compacting..." message, sets `isCompacting = true` (disables input) |
| `compaction_complete` | Rebuilds `messageHistory` from compacted messages, shows summary notification, refreshes cache viewer |
| `compaction_error` | Updates the "Compacting..." message with error, re-enables input |

If `case === 'none'`, the "Compacting..." message is silently removed.

## Token Usage HUD

### Server-Side (`_print_streaming_hud`)

Prints a detailed terminal HUD after each response:

- Per-tier cache block visualization with content descriptions.
- Token breakdown: system, symbol map, files, history.
- Last request: prompt in, completion out, cache hit/write.
- Session totals.

Returns a breakdown dict attached to the `streamComplete` result.

### Client-Side (`_showHud` / `HudTemplate`)

Renders a floating overlay in the bottom-right of the prompt view:

- **Context Breakdown** â€” System, symbol map, files, history tokens with totals.
- **Cache Tiers** â€” Per-tier rows with contents (sys, legend, symbols, files, history) and token counts. Color-coded dots: â— cached, â—‹ uncached.
- **Cache hit percentage** â€” Prominent badge, color-coded (green >50%, amber >20%, red otherwise).
- **This Request** â€” Prompt and completion tokens with cache hit/write details.
- **History status** â€” Token count vs threshold with warning coloring at 80%/95%.
- **Tier Changes** â€” Promotions (ğŸ“ˆ) and demotions (ğŸ“‰) with item names.
- **Session Total** â€” Cumulative in/out/total.

Auto-hides after 8 seconds. Stays visible while mouse hovers (2-second timeout after mouse leaves).

## Error Handling

### File Validation

Before building messages, all file paths are validated:

- **Not found** â€” Collected into `invalid_files`.
- **Binary** â€” Collected into `binary_files`.
- If any problematic files, `streamComplete` is sent with an error. The client auto-deselects these files from the picker.

### Streaming Errors

- **Exception in `_stream_chat`** â€” Caught, traceback printed, `streamComplete` sent with `error` field.
- **Chunk send failure** â€” Logged but not fatal (chunks are fire-and-forget).
- **`streamComplete` timeout** â€” 5-second `asyncio.wait_for` timeout, logged as warning.

### Client Watchdog

A 5-minute timeout (`_startStreamingWatchdog`) forces recovery if `streamComplete` is never received:

- Sets `isStreaming = false`.
- Clears `_streamingRequests`.
- Shows a timeout error message.

## Conversation History Management

### Context Manager

`ContextManager` (`ac/context/manager.py`) is the single source of truth for conversation history:

- **`add_exchange(user, assistant)`** â€” Appends both messages to `_history`.
- **`get_history()`** â€” Returns a copy of the history list.
- **`clear_history()`** â€” Empties history and purges `history:*` entries from stability tracker.
- **`history_token_count()`** â€” Counts tokens in current history.
- **`max_history_tokens`** â€” Budget set to `max_input_tokens // 16`.

### History Store

`HistoryStore` (`ac/history/history_store.py`) provides persistent session-based storage:

- Messages written to `{repo}/.aicoder/history.jsonl`.
- Session IDs group related messages.
- Supports search, session listing, and session loading.

The `HistoryMixin` on `LiteLLM` provides convenience methods (`store_user_message`, `store_assistant_message`, `load_session_into_context`).

### Session Loading

`load_session_into_context(session_id)`:

1. Retrieves messages from `HistoryStore`.
2. Clears `ContextManager` history.
3. Populates `ContextManager` with loaded messages.
4. Sets `HistoryStore._current_session_id` so new messages continue in the loaded session.

On the client side, `handleLoadSession` clears `messageHistory`, adds each loaded message, scrolls to bottom, and refreshes the cache viewer.

## Dual History Stores

The system maintains two parallel history representations:

| Store | Purpose | Mutated By |
|-------|---------|------------|
| `ContextManager._history` | Token counting, message assembly, compaction | `add_exchange`, `set_history`, `clear_history` |
| `HistoryStore` (JSONL) | Persistence, session browsing, search | `store_user_message`, `store_assistant_message` |

Both are updated on each exchange. The `ContextManager` history drives what the LLM sees; the `HistoryStore` provides cross-session persistence and browsing.
