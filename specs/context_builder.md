# Context Builder / Tiered Message Assembly

This spec covers how LLM request messages are assembled from system prompts,
symbol maps, files, conversation history, and URL context — organized into
cache-tiered blocks for efficient prompt caching.

## Overview

Every streaming chat request constructs a message array where content is
stratified into stability tiers. Stable content (unchanged across many
requests) is grouped into cacheable blocks. Volatile content sits in
uncached active blocks. This maximizes prompt cache hit rates while keeping
the full context available to the LLM.

```
messages[] =
  [0]  system    L0: system prompt + legend + L0 symbols + L0 files  [cached]
  [1]  user      L0 history msg 0                                    │
  [2]  assistant L0 history msg 1                                    [cached]
  ───────────────────────────────────────────────────────────────────
  [3]  user      L1: symbols + files                                 │
  [4]  assistant "Ok."                                               │
  [5]  user      L1 history msg 2                                    │
  [6]  assistant L1 history msg 3                                    [cached]
  ───────────────────────────────────────────────────────────────────
  [7]  user      L2: symbols + files                                 [cached]
  ───────────────────────────────────────────────────────────────────
  [8]  user      L3: symbols + files                                 │
  [9]  assistant "Ok."                                               │
  [10] user      L3 history msg 4                                    │
  [11] assistant L3 history msg 5                                    [cached]
  ───────────────────────────────────────────────────────────────────
  [12] user      file tree                                           │
  [13] assistant "Ok."                                               │
  [14] user      URL context                                         │
  [15] assistant "Ok, I've reviewed the URL content."                │
  [16] user      active files                                        │
  [17] assistant "Ok."                                               │
  [18] user      active history msg 6                                │
  [19] assistant active history msg 7                                │
  [20] user      current prompt                                      uncached
```

## Architecture

Message assembly is split across two classes via mixin composition:

- **`ContextBuilderMixin`** (`ac/llm/context_builder.py`): Reusable helpers
  for organizing content by tier. Used by both streaming chat and the
  context breakdown API.
- **`StreamingMixin`** (`ac/llm/streaming.py`): Streaming-specific assembly
  in `_build_streaming_messages()` that calls `ContextBuilderMixin` methods
  and produces the final messages array.

Both are mixed into `LiteLLM`.

## Tier System

Five tiers ordered from most stable to most volatile:

| Tier | Threshold | Description | Cached |
|------|-----------|-------------|--------|
| L0 | 12+ responses | Most stable. System prompt, legend, core symbols/files | Yes |
| L1 | 9+ responses | Very stable symbols and files | Yes |
| L2 | 6+ responses | Stable symbols and files | Yes |
| L3 | 3+ responses | Moderately stable. Entry tier for newly graduated items | Yes |
| active | — | Recently changed or new. Current user files, recent history | No |

Items are assigned to tiers by the `StabilityTracker` based on how many
consecutive responses they have appeared unchanged. See
[Cache Management](cache_management.md) for promotion/demotion rules.

## Content Types

Four types of content are distributed across tiers:

### 1. Symbol Map Entries

Compact representations of file symbols (classes, functions, imports,
references). Generated by `format_symbol_blocks_by_tier()` from the symbol
index.

**Key rule:** Symbol map entries for files in active context are
**excluded**. When a file's full content is included (because the user
selected it), its symbol entry is redundant — the LLM has the actual code.

### 2. File Contents

Full file contents in fenced code blocks:

```
path/to/file.py
\`\`\`
<file content>
\`\`\`
```

Files are organized by tier based on stability. Each tier uses a distinct
header to help the LLM understand the content's role:

| Tier | Header |
|------|--------|
| L0 | `# Reference Files (Stable)` |
| L1 | `# Reference Files` |
| L2 | `# Reference Files (L2)` |
| L3 | `# Reference Files (L3)` |
| active | `# Working Files` |

### 3. Conversation History

User/assistant message pairs from previous exchanges. Distributed across
tiers by the stability tracker — old stable exchanges cache in lower tiers,
recent ones stay active.

History messages are always sent as **native message pairs** (individual
`{role, content}` dicts), not formatted as markdown inside a wrapper
message. This preserves the LLM's understanding of conversation structure.

### 4. URL Context

Content fetched from URLs mentioned in conversation. Always placed in the
active tier (not cached). Limited to 3 URLs per message. Each URL's content
is formatted via `URLContent.format_for_prompt()`.

## Assembly Pipeline

### Entry Point

```python
_build_streaming_messages(
    user_prompt, file_paths, images, use_repo_map, url_context=None
) -> (messages, user_text, context_map_tokens, tier_info)
```

Returns:
- `messages`: The complete message array for the LLM API call.
- `user_text`: The user's prompt text (for history storage).
- `context_map_tokens`: Total symbol map tokens across all tiers.
- `tier_info`: Per-tier token counts and content summaries for the HUD.

### Step 1: Determine Active Context Files

```python
active_context_files = set(file_paths)  # normalized to forward slashes
```

These files will have full content included. Their symbol map entries are
excluded from all tiers.

### Step 2: Get File Tiers

```python
file_tiers = self._get_file_tiers(file_paths)
```

`_get_file_tiers()` queries the stability tracker:
- Tracked files → assigned tier from tracker.
- Untracked files → default to `active`.
- If no stability tracker exists → all files go to `active`.

Returns `{tier: [file_paths]}` for all five tiers.

### Step 3: Build System Prompt

```python
system_text = build_system_prompt()  # system.md + system_extra.md
```

Always placed at the start of the L0 block. See
[Prompt Construction](prompt_construction.md).

### Step 4: Get Symbol Map Data

```python
symbol_map_content, symbol_files_by_tier, legend, legend_tokens = \
    self._get_symbol_map_data(active_context_files, file_paths)
```

This method (on `ContextBuilderMixin`):

1. Gets all trackable files from the repository.
2. Indexes them via `SymbolIndex.index_files()`.
3. Builds cross-file references via `build_references()`.
4. **First request only:** Initializes the stability tracker from the
   reference graph using `initialize_from_clusters()` (graph clustering)
   or `initialize_from_refs()` (percentile fallback).
5. Computes path aliases for the legend.
6. Gets symbol tiers via `_get_symbol_tiers()`.
7. Formats symbol blocks per tier via `format_symbol_blocks_by_tier()`.

Returns content already split by tier, ready for insertion into cache
blocks.

### Step 5: Get History Tiers

```python
history_tiers = self._get_history_tiers()
```

Queries the stability tracker for `history:N` item assignments. Returns
`{tier: [indices]}` where indices are positions in
`self.conversation_history`. Untracked messages default to `active`.
Indices within each tier are sorted ascending to maintain message order.

### Step 6: Assemble L0 Block

The L0 block is special — it is the system message:

```python
l0_content = system_text
             + REPO_MAP_HEADER + legend       # if legend exists
             + L0 symbol map content          # if any
             + L0 file contents               # if any
```

**With L0 history:**
```
messages[0] = {role: "system", content: l0_content}       # no cache_control
messages[1..N] = L0 history pairs                         # native messages
messages[N].cache_control = ephemeral                     # on LAST message
```

**Without L0 history:**
```
messages[0] = {role: "system", content: [{
    type: "text",
    text: l0_content,
    cache_control: {type: "ephemeral"}
}]}
```

The difference is where `cache_control` is placed. When history is present,
the cache boundary extends through the last history message. When absent,
only the system message is cached.

### Step 7: Assemble L1/L2/L3 Blocks

Each tier is assembled by `_build_tier_cache_block()`:

```python
result = self._build_tier_cache_block(
    tier, symbol_map_content, symbol_files_by_tier,
    file_tiers, tier_info, file_header,
    history_tiers=history_tiers
)
```

Returns `{messages, history_messages, symbol_tokens}` or `None` if the
tier is entirely empty.

**Content assembly per tier:**

1. Symbol map content for this tier (prefixed with
   `# Repository Structure (continued)`).
2. File contents for this tier (with tier-specific header).
3. These are combined into a single user message + `"Ok."` assistant reply.
4. History messages for this tier as native pairs.
5. `cache_control` is placed on the **last** message in the combined
   sequence (symbols/files pair + history pairs).

**Empty tier handling:** If a tier has no symbols, no files, and no history,
it is skipped entirely. The `tier_info['empty_tiers']` counter tracks how
many tiers were skipped, accumulated across the session for the HUD.

### Step 8: Active Content

Active content is not cached. Added in order:

1. **File tree** (if `use_repo_map`): User message with full repository
   file listing + `"Ok."` assistant reply.
2. **URL context**: User message with fetched URL contents + `"Ok, I've
   reviewed the URL content."` assistant reply.
3. **Active files**: User message with file contents using
   `# Working Files` header + `"Ok."` assistant reply.
4. **Active history**: Individual native message pairs, maintaining
   original order.
5. **User prompt**: The current user message (with optional images).

### Step 9: Track State

```python
self._session_empty_tier_count += tier_info['empty_tiers']
```

Accumulated for HUD display.

## Cache Control Placement

Anthropic-style prompt caching uses `cache_control: {type: "ephemeral"}`
markers to indicate cache block boundaries. The key placement rules:

1. **One marker per tier** — placed on the last message in each tier's
   sequence.
2. **Static method** `_apply_cache_control(message)` handles the wrapping:
   - If content is already a list (structured), adds `cache_control` to the
     last text block.
   - If content is a plain string, wraps in structured format:
     `[{type: "text", text: content, cache_control: {type: "ephemeral"}}]`.
3. **Active content has no markers** — it changes every request so caching
   would waste write tokens.

## ContextBuilderMixin Helpers

### _is_error_response(result)

Checks if a repo layer result is an error dict (`{error: ...}`).

### _get_file_content_safe(path, version)

Gets file content from repo with full error handling. Returns `None` on any
failure (missing file, error dict, exception). Single place for the
error-dict pattern.

### _safe_count_tokens(content)

Counts tokens via `ContextManager.count_tokens()` with error handling.
Returns `0` on any failure.

### _get_stability_tracker()

Returns `context_manager.cache_stability` or `None`.

### _get_file_tiers(file_paths)

Organizes file paths into tiers via the stability tracker. Untracked files
default to `active`. No tracker → all `active`.

### _get_symbol_tiers(symbols_by_file, exclude_files)

Organizes symbol entries into tiers. Symbol items use `symbol:` prefix in
the tracker. Untracked symbols default to `L3` (not `active` — symbols
start cached). Excludes files in `exclude_files` (active context).

### _get_symbol_map_data(active_context_files, file_paths)

Full symbol map pipeline: index → references → stability init → aliases →
tier formatting. Returns `(content_by_tier, files_by_tier, legend,
legend_tokens)`.

### _initialize_stability_from_refs(...)

First-request initialization of the stability tracker. Two strategies:

1. **Graph clustering** (preferred): Uses bidirectional reference edges to
   find mutually coupled file clusters. `find_connected_components()` groups
   files, then `initialize_from_clusters()` distributes clusters across
   tiers via greedy bin-packing.
2. **Percentile fallback**: Sorts files by reference count and distributes
   across tiers proportionally.

Both strategies use a token estimator that formats each symbol entry and
counts tokens, ensuring tier sizes respect `cache_target_tokens`.

### _build_file_items(file_paths)

Builds file item dicts with token counts and stability info for the context
breakdown API. Returns `(items, total_tokens)`.

### _build_symbol_items(file_paths)

Builds symbol item dicts with stability info. Returns list of dicts.

### _build_history_items(message_indices)

Builds history item dicts with token counts and stability info. Returns
`(items, total_tokens)`.

### _build_url_items(fetched_urls)

Builds URL item dicts with token counts. Includes header overhead tokens.
Returns `(items, total_tokens)`.

### _make_tier_info()

Creates the empty tier info structure used by both streaming and context
breakdown:

```python
{
    'L0':     {tokens, symbols, files, history, has_system, has_legend, has_tree},
    'L1':     {tokens, symbols, files, history, has_tree},
    'L2':     {tokens, symbols, files, history, has_tree},
    'L3':     {tokens, symbols, files, history, has_tree},
    'active': {tokens, symbols, files, history, has_tree, has_urls, has_history},
    'empty_tiers': 0,
}
```

## Context Breakdown API

`LiteLLM.get_context_breakdown()` uses `ContextBuilderMixin` to build the
same tier structure as streaming, but returns it as a JSON-serializable
dict for the frontend cache viewer instead of assembling LLM messages.

It calls the same helpers (`_get_file_tiers`, `_get_symbol_map_data`,
`_get_history_tiers`, `_build_file_items`, etc.) ensuring the breakdown
display matches what the LLM actually receives.

## File Formatting

`_format_files_for_cache(file_paths, header)` formats files for inclusion
in a cache block:

```
<header>

path/to/first.py
\`\`\`
<content>
\`\`\`

path/to/second.py
\`\`\`
<content>
\`\`\`
```

Returns empty string if no files have readable content (header-only output
is suppressed).

## Section Headers

Constants defined in `streaming.py`:

| Constant | Content |
|----------|---------|
| `REPO_MAP_HEADER` | `# Repository Structure\n\nBelow is a map...` |
| `REPO_MAP_CONTINUATION` | `# Repository Structure (continued)\n\n` |
| `FILE_TREE_HEADER` | `# Repository Files\n\nComplete list of files...` |
| `URL_CONTEXT_HEADER` | `# URL Context\n\nThe following content...` |
| `FILES_L0_HEADER` | `# Reference Files (Stable)\n\n...` |
| `FILES_L1_HEADER` | `# Reference Files\n\n...` |
| `FILES_L2_HEADER` | `# Reference Files (L2)\n\n...` |
| `FILES_L3_HEADER` | `# Reference Files (L3)\n\n...` |
| `FILES_ACTIVE_HEADER` | `# Working Files\n\n...` |

## HUD Output

`_print_streaming_hud()` prints a terminal HUD after each streaming
request with:

- Per-category token breakdown (system, symbol map, files, history).
- Model name and token limits.
- Last request input/output/cache stats.
- Session cumulative totals.

`_print_cache_blocks()` prints a visual cache block diagram showing each
tier's token count, contents, cache hit percentage, and empty tier count.

Both methods also return structured data for the frontend HUD
(`token_usage` in the `streamComplete` result).

## Post-Response Stability Update

After streaming completes, `_update_cache_stability()` updates the
stability tracker with the current request's items. This is covered in
detail in [Cache Management](cache_management.md) and
[Streaming Chat](streaming_chat.md). The context builder is not
responsible for the update — it only reads tier assignments during
assembly.
